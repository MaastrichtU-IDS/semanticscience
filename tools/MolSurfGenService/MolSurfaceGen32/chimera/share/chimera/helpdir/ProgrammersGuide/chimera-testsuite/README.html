<html>
<head> <title> Chimera Test Suite Documentation </title> </head>

<body>

<center><h3>Chimera Test Suite Documentation</h3></center>


<p>This document contains information about using the Chimera test suite, including the writing of tests, 
and the test suite's image comparison capabilities. It is divided into several sections:
<ul>
  <li><a href="#roadmap">Roadmap</a> (high-level overview of all component directories in test suite) </li>
  <li><a href="#test_structure">Test structure</a> (discusses implementation details for 
  Chimera's tests, using Python's unit testing framework) </li>
  <li><a href="#adding_tests">Adding tests</a> (how to incorporate a test into Chimera test suite)</li> 
  <li><a href="#running_tests">Running tests</a> (how to run the Chimera test suite)</li>
  <li><a href="#image_comparison">Image comparison</a> (covers the test suite's image 
  capture and comparison capabilities)</li>
</ul>
</p>

<hr width="100%">

<a name="roadmap">
<h3><u>Roadmap</u></h3>
(<b>Note:</b>&nbsp; 'CHIMERA' in all caps, refers to the root of the Chimera CVS development tree.)
<br<br>

<b>CHIMERA/test/</b>
<pre>	root directory for Chimera testing framework</pre>

<b>CHIMERA/test/README</b>
<pre>	[this file!] contains instructions on the Chimera testing structure, how to run the tests, how to add new tests.</pre>

<b>CHIMERA/test/tools/</b>
<pre>	directory contains several scripts and modules that manage and run tests</pre>

<b>CHIMERA/test/tools/tester.py</b>
<pre>	runs the development version of Chimera (chimera-build/PLATFORM/build/chimera) on specified platforms
        with specified tests, and posts results on the chimera testsuite intranet site. 
</pre>

<b>CHIMERA/test/tools/generateTestFile.py</b>
<pre>	generates a file that tells Chimera which tests to run, and prints out location of file.
        this script can also be used to run the development version of chimera, but produces only a 
        raw output file.
</pre>			

<b>CHIMERA/test/tools/TestUtils.py</b>
<pre>	contains several functions and classes that test-writers may find helpful to use in writing their tests</pre>

<b>CHIMERA/test/pytests/</b>
<pre>	directory contains all the unit tests that can be run by the testing framework. 
        passing an argument of <i>all</i> to the tester script will result in all of these 
        tests being run
</pre>

<b>CHIMERA/test/data/</b>
<pre>	directory contains data files used by multiple tests</pre>

<b>CHIMERA/test/images/</b>
<pre>        directory contains images generated by test suite. Each platform has its own subdirectory within this directory.</pre>

<br><br>
<hr width="100%">
<a name="test_structure">
<h3><u>Test Structure</u></h3>

<p>There are several levels of testing 'layers' involved the Chimera test suite. 
This provides flexibility for arbitrarily grouping tests to test complex 
usage patterns in Chimera. Here is an overview:</p>
<pre>
<b>TestGroup</b>
    |  
    | 
    | <i>contains one or many</i>
    |
    |____> <b>TestCase</b>
             |
             |
             | <i>contains one or many</i>
             |
             |___> <b>unit test</b> 

</pre>

<p>The most basic unit of testing is the actual <b>unit test</b>. A unit test is a 
self-contained entity and it should encapsulate testing one very specific functional unit of a program, 
and then checking to make sure that whatever you would expect to happen as a result, actually did happen. 
Unit tests cannot live on their own however; they must exist as part of a <b>TestCase</b>.

<br><br>
<h4>TestCase</h4>
<p>A <b>TestCase</b> class can contain one or many unit tests, in addition to any
housekeeping code that deals with setting up before, and cleaning up after,
the test has run (or before/after each test has run, if there are several).
<br>
For example, here's a pseudocode representation of a TestCase:
</p>


<pre>
class ExampleTestCase(TestCase)

   def <b>setUp</b>():
     """This method is run before the actual test ('runTest') is run
     This method 'sets the stage' the test to run. In this example, this
     includes opening the model 3fx2.pdb .
     """
     open 3fx2.pdb
	
   def <b>runTest</b>():
     """The actual test. This method comprises the actual content of the
     test.
     """

     doMidasCommand('repr stick')
     for each atom in 3fx2:
	if atom is not stick:
		TEST FAILS!!!!
     TEST SUCCEEED!

   def <b>tearDown</b>():
     """This method performs any necessary cleanup after the test has completed.
     In this example, it closes the molecule that was opened in the 'setUp'
     method.
     """
     close 3fx2.pdb
</pre>


A TestCase can have <code>setUp</code> and <code>tearDown</code> methods,  which execute before and after 
(respectively) each test in the TestCase is run. So, if the TestCase has multiple <code>testWhatever</code>
methods, <code>setUp</code> and <code>tearDown</code> will run before and after each <code>testWhatever</code> method runs.


<p> The above TestCase only has one unit test. It is (<b>must be</b>) contained in a method named
<code>runTest</code>. However, it is also possible for a TestCase to have more than one unit tests. In this
case, the TestCase should not have a <code>runTest</code> method, but instead should have one or more
methods that start with the string <code>test</code>. The above example TestCase with several unit tests would
look like:

<pre>
class Example2TestCase(TestCase)

   def <b>setUp</b>():
     """This method is run before the actual test ('runTest') is run
     This method 'sets the stage' the test to run. In this example, this
     includes opening the model 3fx2.pdb .
     """
     open 3fx2.pdb
	
   def <b>testStickRepr</b>():
     """An actual test. Tests Chimera's stick atom representation"""

     doMidasCommand('repr stick')
     for each atom in 3fx2:
	if atom is not stick:
		TEST FAILS!!!!
     TEST SUCCEEED!

   def <b>testAtomColor</b>():
     """Tests the color command"""

     doMidasCommand('color blue')
     for each atom in 3fx2:
	if atom is not blue:
		TEST FAILS!!!!
     TEST SUCCEEED!

   def <b>testUndisp</b>():
     """Tests the ~disp command"""

     doMidasCommand('~disp')
     for each atom in 3fx2:
	if atom is not hidden:
		TEST FAILS!!!!
     TEST SUCCEEED!

   def <b>tearDown</b>():
     """This method performs any necessary cleanup after the test has completed.
     In this example, it closes the molecule that was opened in the 'setUp'
     method.
     """
     close 3fx2.pdb
</pre>

 
<p>Test failure or success can be determined in several ways. 
If the test is testing for a particular value of something, you can use one of several assertions 
which will cause a test to fail if the required conditions are not met (or if they are, depending on the assertion). 
Complete details on using these assertions can be found in the 
<a href="http://www.python.org/doc/current/lib/testcase-objects.html">Python unittest documentation</a>  
(or see any of the test files in <i>CHIMERA/test/pytests/</i> for examples of these assertion statements)
A test will be considered <b>failed</b> if one of the assert statements in the test fails; i.e. if what you expect
to happen doesn't happen, the test has failed. If an uncaught exception is raised during the execution of that test,
then the test is considered to have encountered an <b>error</b>; i.e. if something breaks that you did't expect to 
break, it is considered an error. An error usually means that there is something wrong with the test, not with
Chimera.If a test completes running without failing any assert statements, and without raising any uncaught 
exceptions, it will be considered <b>passed</b>.</p>

The file TestUtils.py has a class called <code>Open3fx2TestCase</code> which defines <code>setUp</code> and
<code>tearDown</code> methods to manage the opening and closing of the 3fx2 molecule. 
It can be used as a superclass, by implementing a <code>runTest</code> method, 
or several <code>testSomething</code> methods.


<br><br>
<h4>TestGroup</h4>

<p>TestCases can be grouped into a <b>TestGroup</b>. A TestGroup contains one or more TestCases:</p>

<pre>
class ExampleTestGroup:
    
    def <b>setUpOnce</b>():
       ## perform some set up before the tests are run
	
    def <b>tearDownOnce</b>():
       ## clean up after all the tests are run

    class ExampleTC1(TestCase):
	 def <b>setUp</b>():
           ## set up specific to this test 
    
         def <b>runTest</b>():
	   ## do some stuff here

	 def <b>tearDown</b>():
           ## clean up specific to this test

     class ExampleTC2(TestCase):
         def <b>setUp</b>():
           ## set up specific to this test

         def <b>runTest</b>():
            ## do other stuff here

	 def <b>tearDown</b>():
            ## clean up specific to this test
</pre>


<p>A TestGroup can have methods named <code>setUpOnce</code> and <code>tearDownOnce</code>, which  are 
run <code>one time</code> before and after (respectively) 
<code>all</code> tests are run. 
So in the above example, the order of execution would be 
<p><pre>
ExampleTestGroup.setUpOnce, 
ExampleTC1.setUp, ExampleTC1.runTest,  ExampleTC1.tearDown, 
ExampleTC2.setUp, ExampleT2.runTest,  ExampleTC2.tearDown
ExampleTestGroup.tearDownOnce 
</pre></p>

<p>The file TestUtils.py provides a class called <code>Open3fx2TestGroup</code>
 which you can use as a superclass when writing your own tests. 
This provides <code>setUpOnce</code> and <code>tearDownOnce</code> methods that 
manage the opening and closing of the  3FX2.pdb molecule . 

<p>This enables you to  simulate performing multiple actions on a model, while testing for
a condition at each intermediary step. </p>


<br>
<hr width="100%">
<a name="adding_tests">
<h3><u>Adding tests</u></h3>
<p>
In order to add a test to the test suite:
<ol>
   <li>create either a standalone file or a package (if you need included data files) whose name starts with
      'pyt_' (eg. 'pyt_MidasCommand.py'). Currently the testing script looks for files/packages with that substring
      in their name. Place this module in the <i>CHIMERA/test/pytests/</i> directory.</li>
   
   <li>Give the file or package an attribute named <code>RUNME</code> (if you are making a package, this attribute should
      go in the <i>__init__.py</i> file of the package) 
      This should be set to <code>True</code> if the test is to be run or <code>False</code> if it is to be left out.
      This provides an easy way of turning on/off certain tests.</li> 

   <li>Make sure that your test subclasses from one of the superclasses in <i>TestUtils.py</i> and that it is accessible
      at the top level of the module.</li>
</ol>

<p>See <i>CHIMERA/test/pytests/pyt_MidasCommands.py</i> for a documented template.</p>

<br><br>
<h4>Test Ordering</h4>
<p>There are several notions of test ordering. Several situations are possible:</p>

<ol>

<li><b>Random</b> - you want to run the tests in a totally random order   
There is a way to randomize the tests. See below under <i>Running Tests</i> about the <i>-rand</i> argument 
to the <code>generateTestFile.py</code> script.
</li>

<li><b>Global order</b> - you want to run the test modules (i.e. each file/package in pytests) in a specific order. 
If desired, you can provide an attribute called 'NTH' at the top level of a test and assign it the number in which 
you want that test to run. For example, if you want the tests pyt_MidasCommands.py to run first
(i.e. say, before the test in pyt_AtomSpec.py), give the pyt_MidasCommands.py  a top-level attribute
named NTH, and assign it a a number. 

e.g.  somewhere near the top of the pyt_MidasCommands.py, would be a line:
<p>
<code>NTH=1</code>
</p>
Test modules that are not given a global ordering will run after tests that are.
</li>

<li><b>Local order</b> - you want to run each individual TestCase within a TestGroup in order.
In this case, you can provide a TestGroup-level variable called named 'ORDER' that is assigned 
a list of names of TestCases. The TestCases will be run in the order specified in the list.
(see pyt_ResidueAttrTests.py or pyt_DemoTests.py for examples).
</li>

</ol>

<b>Note:</b> there is currently no mechanism to run all the unit tests in a TestCase in a specific order


<br><br>
<hr width="100%">
<a name="running_tests">
<h3><u>Running Tests</u></h3>

<p>There are three main ways to run the test suite:
<ol>
<li>From <b>within Chimera</b>, using the Test Suite extension
<li>Using the <b>tester.py</b> script, which is used to run the test
suite on multiple machines against the latest development version
of Chimera, and output the test results to the 
<a href="https://www.cgl.ucsf.edu/intranet/chimera-testsuite/index.html">
Chimera testsuite results</a>  page on the CGL Intranet site.
<li>Using the <b>generateTestFile.py</b> script, which can be used to run the
test suite locally on an arbitrary installation of Chimera, and output
the results to a user-defined output file.
</ol>


<h4><u>1. Test Suite extension</u></h4>
<p>The test suite extension can be used to run the test suite from within
Chimera. Code for the extension is checked into the Chimera CVS tree at 
<pre>   chimera/test/TestSuiteExt</pre>
In order for Chimera to pick up this extension, you must put your the
directory <b>above</b> the <code>TestSuiteExt</code> (<i>i.e.</i>
<code>chimera/test</code>) on your Chimera extension search path. 
This will add the item <code>Tools/Utilities/Run Test Suite</code> to
the Chimera menu system.</p>

<p>Invoking the test suite extension brings up a window with a list of 
tests on the left side, and an empty output log on the right side. This 
list is populated by looking for all the files/directories in 
<code>chimera/test/pytests</code> that have the prefix <code>'pyt_'</code> .
Clicking on any of the tests in the list will display a description of 
that test in the lower left-hand corner of the window. Multiple tests
can be selected by <code>Shift-</code> or <code>Ctrl-</code>clicking.
Pressing <b>Enter</b> or clicking on the <b>Run</b> button
will run the selected tests, and display output to the right-hand 
side of the dialog. Choosing the list item labeled <b>all</b> will run
all test suite tests that have their RUNME attribute set to True
(see <a href="#adding_tests">this section</a> for more information about 
the <code>RUNME</code> attribute).
<b>Clear output</b> can be used to clear the contents of the output log, 
and <b>Save</b> can be used to save it to a file.</p>

<p>The test suite extension is the easiest way to run just a couple tests,
or debug new unit tests as you write them.</p> 

<h4><u>2. generateTestFile.py - run the t.s. on a local installation</u></h4>
<p>The purpose of the <code>generateTestFile.py</code> script is to produce a python file
('test file') that can be passed in to Chimera, which will run a series of tests (defined by input options 
to <code>generateTestFile.py</code>), and produce an output file containing results from the 
tests that were run. Actually, when run in Chimera, the 'test files' generated by this script
will produce three different output files, all which are named according to the supplied output 
file name. So if you gave an argument of
<code>test.out</code> to the <code>-o</code> option, these three output files would be
<ul>
<li><b>test.out_SUM</b>  - contains a concise html table of all the tests that were run, and for 
each one, how many errors occured and how many failures occurred. 
<li><b>test.out_STAT</b> small file with one line, which contains three whitespace-seperated
numbers - the total # tests run, the total number of errors, and the total number of failures  
<li><b>test.out.html</b> - contains the actual test results nicely formatted in html. The 
summary table that is written to test.out_SUM is actually prepended to the raw results, with
each test listing in the table linking to the raw results further on down the page. 
</ul>

<p>The standard usage scenario for this script is that you have some installation of Chimera
on your machine, let's say it's in <code>/tmp/dan/chimera-NEW</code>, and you want to
run the test suite against it. This can be done by running chimera, and using the <code>generateTestFile.py</code>
script to generate an input file. <code>generateTestFile.py</code> outputs the path to the generated test
file on std out; you can use backquotes (`) to capture the output of the script (<i>i.e.</i> the path
to the test file) and pass it in to Chimera:
<pre>
      /tmp/dan/chimera-NEW/bin/chimera `./generateTestFile.py -t all -o results.out`
</pre>

<p>This tells <code>generateTestFile.py</code> to make a Python file that has all the code necessary
to run the test suite on all the tests and write the output to a file named <code>results.out</code>
in the current directory. The path to this test file is printed to standard out. Then, Chimera is invoked with 
this file (actually a python file) as input.</p>


<!--While it can be used to run the development version also (by passing it the
<i>-r</i> option), this script is more useful when you wish to run the testsuite on a 
non-development version of Chimera (e.g. regression tests on a previous release you have
installed in your home directory). -->




<br><br>
<b>USAGE</b>:
<pre>         generateTestFile.py [-r] [-p platform_directory] [-rand] -o output_file -t test_list</pre>

<b>DESCRIPTION</b>:
<pre>        -r          don't just generate a test file; actually run Chimera against the specified tests. 
		    The location of the Chimera that is actually run depends on the -p option, if
                    it is specified. If the -p option is <b>not</b> given, the following mapping specifies 
	            which Chimera will be run, based on which system the generateTestFile.py script is run on:
                    
                    /usr/local/src/staff/chimera-build/Darwin-X11/build  ->  Mac OS X
		    /usr/local/src/staff/chimera-build/Linux-X11/build   ->  Linux
		    /usr/local/src/staff/chimera-build/IRIX-X11/build    ->  SGI IRIX
		    /usr/local/src/staff/chimera-build/OSF1-X11/build    ->  Alpha Tru64

		     If the -r option is not specified, the script will just print out the path
                     of the generated test input file. This file can then be passed to Chimera
 		     (its just a python file) to run the specified tests with all the desired options.
		    

        -p  platform_directory 
                    specify which build of Chimera to run.
                    The argument must correspond to a name of one of the directories in 
  	            <code>/usr/local/src/staff/chimera-build/</code> . Currently this is one of:
                    <b>Darwin-X11</b>, <b>IRIX-X11</b>, <b>Linux-X11</b>, 
		    <b>OSF1-X11</b>, <b>RedHat8-X11</b> 
                   
                    If this option is not specified, a directory will be chosen based
                    on the content of Python's <code>sys.platform</code> . This option is only really
                    useful for specifying which Linux build to run (<b>Linux-X11</b> specifies the
		    distribution built on Red Hat 7.1, and <b>RedHat8-X11</b> specifies the distribution
		    build on RH 8.0 ) if you're on a Linux machine.

         
         -rand      run tests in a random order (module level)  


         -o  output_file
                    Write test results to output_file


         -t  tests_list
	            A whitespace seperated list of tests to run, or <code>all</code> to run all
 		    tests whose RUNME attribute is set to True. This script determines which files 
                    contain viable tests by looking in the test directory (<code>chimera/test/pytests</code>) 
                    for modules or packages whose name starts with the prefix '_pyt'. If a list of test names
                    is given, elements of the list correspond to names of modules or packages in the <code>CHIMERA/test/pytests</code>
                    directory (minus the <code>.py</code> suffix). 
	

<b>Example Usage</b>:
	<code><b>%</b> ./generateTestFile.py -rand -o test.output  -t all</code>
	   will run the corresponding build version of chimera, 
           running test modules in a random order. Output goes to a file 'test.output')
	
	<code><b>%</b> /usr/local/chimera `./generateTestFile -o test.output -t pyt_MidasCommands pyt_AtomTrigger`</code>
	   will run <code>/usr/local/chimera</code>, only running the tests contained in the files <code>pyt_MidasCommands.py</code>  
	   and <code>pyt_AtomTrigger.py</code> . Output goes to <code>test.output</code>. Notice the `backticks` around the 
           generateTestFile command and arguments, because the script prints the location of the test file to standard out.
</pre>

<p>
<b>Note</b>: make sure that you are in the <code>chimera/test/tools</code> directory when you run this script. 
Otherwise, bad things will happen! 
</p>



<h4><u>3. tester.py - automatically run t.s. and post results on the web</u></h4>

<p><code>tester.py</code> is a slightly higher level tool than <code>generateTestFile.py</code>, because
in addition to generating a test file or running Chimera against the specified tests, the <code>tester.py</code> 
script post-processes the test output, and drops the results into a several nicely formatted files on the 
<a href="index.html">Chimera testsuite intranet site</a>. This script should be used
whenever you want to test the development build of Chimera (chimera-build/PLATFORM/build/chimera).
<b>Note</b> that this script must be run from socrates to function correctly. There is an option
(see below) to specify on which X-server to display the Chimera window (this would most likely be your
desktop machine from which you logged into socrates).</p>


<br><br>
<b>USAGE</b>:
<pre>          tester.py -p platform_list -t test_list -d display_host</pre>
              
<b>DESCRIPTION</b>:
<pre>            -p platform_list
                      a whitespace-seperated list that specifies on which platforms to run the test suite against
                      the development version (chimera-build/PLATFORM/build/chimera) of Chimera. 
                      <i>platform_list</i> can be one or more of the following: <i>Darwin-X11</i>, <i>IRIX-X11</i>, 
                      <i>Linux-X11</i>, <i>OSF1-X11</i>, or <i>RedHat8-X11</i>.
              
            -t test_list
                      a whitespace seperated list of tests to run (actually the name of the containing
                      file, minus the '<code>.py</code>' suffix, e.g., <i>pyt_MidasCommands</i>, or 
                      <i>'all'</i> to run all the tests (those which have the <code>RUNME</code> attribute 
                      set to True)
              
            -d display_host      
                      the name of the host running the X-server on which Chimera should be displayed, e.g.
                      <i>tolkien.cgl.ucsf.edu</i>
</pre>
<b>Caveats for using <code>tester.py</code></b>
<ul>
         <li><b>You must be logged in to socrates</b> to run this script. The script works by ssh<font size=-1>ing</font> into
the desired hosts, running the test suite, producing output, then copying these files back to socrates
where they are processed and dropped into a location where they are viewable on the web. Thus, the script
assumes certain things about where you are in relation to the web server.</li>
         <li><b>It helps to have password-less ssh set up</b> between socrates and the hosts you want to test. This
can be done in a couple different ways, the easiest of which involves generating a d/rsa key and using the
<code>HOME/.ssh/authorized_keys</code> mechanism. However this is not mission-critical , just be prepared to
enter your password into the command line twice for each host you're testing. </li>
<li><b>Make sure you're accepting x connections</b> from the host you're testing (client) to the host you
supplied as an argument to the -d option (server). This can be done using the command
<pre>
    <b>%</b> <code>xhost +client_host</code>
</pre>
on the server host, where <code>client_host</code> is the name of the host you will be testing Chimera on.</li>
</ul>
<pre>
<b>Example Usage</b>:
         
          <code><b>%</b> ./tester.py -p IRIX-X11 -t pyt_AtomSpec  -d tolkien.cgl.ucsf.edu</code>
	  
          will run the build version of Chimera on IRIX against the Atom Specification unit test, 
          and displays the Chimera window on tolkien's X-Server. The output of this test will be posted
          on the chimera test suite intranet site. 
</pre>
<p>
<b>Note</b>: make sure that you are in the <code>chimera/test/tools</code> directory when you run this script. 
Otherwise, bad things will happen! 
</p>



<br><br>
<b>How tester.py posts results to the intranet site</b>
<br>
<p>The mechanism by which <code>tester.py</code> generates test output and images on the host machine, 
copies stuff over to socrates, and finally updates <code>intranet/chimera-testsuite/index.html</code> page  
is pretty convoluted; a brief explanation is in order.</p>

<p>The following procedure is repeated for each platform which is passed into the
<code>tester.py</code> script (with the <code>-p</code> option). This platform will be 
referred to in this discussion as the <i>target platform</i>.

<ol>
<li>Each platform that can be passed in to the script has a designated host
running that platform:
<br><br>
<table border=1>
<tr>
  <th>argument to <code>-p</code> option</th>
  <th> host name </th>
</tr>
<tr>
  <td>OSF1-X11</td>
  <td>socrates.cgl.ucsf.edu</td>
</tr>
<tr>
  <td>Linux-X11</td>
  <td>tolkien.cgl.ucsf.edu</td>
</tr>
<tr>
  <td>RedHat8-X11</td>
  <td>tolkien.cgl.ucsf.edu</td>
</tr>
<tr>
  <td>IRIX-X11</td>
  <td>spinoza.cgl.ucsf.edu</td>
</tr>
<tr>
  <td>Darwin-X11*</td>
  <td>austin.cgl.ucsf.edu</td>
</tr>
<tr>
  <td>Windows-WGL*</td>
  <td>buckley.cgl.ucsf.edu</td>
</tr>
</table>

<p>Note: platforms with an (*) don't really work in this automated framework and must be run
manually (but it would be nice if they did).</p>
</li>

<li>The <code>tester.py</code> script SSHes into the designated host, with an ssh
command like
<pre>
    ssh -X (USER)@(HOST) env DISPLAY=(DISPHOST):0
</pre>

<p> where (USER) is the username of whoever is running this script, (HOST) is the host that
corresponds to the given platform argument, and (DISPHOST) is argument supplied to <code>
tester.py</code> for the <code>-d</code> option (which dictates on what host Chimera should 
display). So you could theoretically be logged into socrates, run Chimera on host A but 
direct it to display on host B.</p>

<p>While on the target host, it changes directory to 
<code>/usr/local/src/staff/(USER)/chimera/test/tools</code> and issues the following command:
<pre>
./generateTestFile.py -r -p (PLATFORM) -o /tmp/out.(PLATFORM) -t (TESTS)
</pre>
<p>This invocation of <code>generateTestFile.py</code> script generates a test file containing 
all the tests passed in to <code>tester.py</code> (this list replaces (TESTS) in the above
example), and runs the platform-specific development version of Chimera 
(note the <code>-r</code> option)  with this test file as an argument. The output of the tests
is written to /tmp/out.(PLATFORM)</p>
</li>

<li>Next, the <code>tester.py</code> script makes a new directory:
<pre>
   socrates:/usr/local/src/www/cgl/secure/datadocs/chimera-testsuite/(PLATFORM)-results/(DATESTAMP)
</pre>
<p>, where (DATESTAMP) in the above example will be named according to the time the tests
were run. For example, if the tests were run on June 16th, 2005 at 1:27 pm (and 12 seconds), 
(DATESTAMP) would be <code>20050616132712</code>.
</li>

<li>The test results are copied from the target platform over to this new directory on 
socrates. The following two files are copied over:
<ul>
<li>out.(PLATFORM).html - the actual test results ( this html file is generated from test results in out.(PLATFORM) )
<li>out.(PLATFORM)_STAT   - log file that contains total # tests run, total # errors, and total
# failures (used to regenerate Chimera test suite index.html page, see below)
</ul>
</li>

</ol>

<p>Once the above procedure has been repeated for all the platforms passed in to the <code>
tester.py</code> script, the Chimera test suite index.html file is updated to reflect the
newly run tests: For each platform that was included in the most recent test run:
<ul> 
<li> its entry in the table is changed to show a faint blue background
<li> it's timestamp is updated
<li> the number of tests run, errors, and failures are updated, and this text is
hyperlinked to the most recent test results (that were copied into the 
<code>chimera-testsuite/(PLATFORM)-results/(DATESTAMP)</code> directory) </li>
</ul>
</p>

<p>The last step is running the image comparison scripts. Some of the tests in the test
suite will actually save images for later comparison, e.g. to test that the ribbon-
rendering code renders ribbons correctly. These images are by default saved to the
directory:
<pre>
   /usr/local/src/staff/(USER)/chimera/test/images/(PLATFORM)
</pre>

The script <code>imgcmp.py</code> is given a list of directories - one for each of
the platforms that were included in this test suite run, and figures out 
which filenames are present in all these directories. The other input into this
script is where it should write the results (output) of the image comparison. The following
path is given:
<pre>
socrates:/usr/local/src/www/cgl/secure/datadocs/chimera-testsuite/image-comparison/index.html
</pre>
<p>This page will contain a table which compares each image on each platform 
produced by the test suite with the same image on all the other platforms that were included
in this run. It is accessible from the main Chimera test suite index page 
(see the next section for more information on image comparison).
</p>


<br>
<hr width="100%">
<a name="image_comparison">
<h3><u>Image Comparison</u></h3>
<h4><code>TestUtils.snapshot</code> function - generating images</h4>
TestUtils.py defines a function called <code>snapshot</code> that can be used to capture an image
of Chimera's graphics window at any time during a test:

<pre>   TestUtils.snapshot(filename="large_match.png", width=512, height=512, format="PNG", supersample=3)</pre>

<pre>
It takes the following arguments:
  <i>filename</i>    - name of file to save image to (<b>required</b>)
  <i>width</i>      - width of image to save  (default = 512 pixels)
  <i>height</i>      - height of image to save (default = 512 pixels)
  <i>format</i>      - image format, can be any format Chimera knows how to save (default = "PNG")
  <i>supersample</i> - how much to supersample when saving image (default = 3)
</pre>

<p>
It is also possible to set the environment variable <b>CHIMERA_IMAGE_DIR</b>, to specify
what directory the tests should save images to. This directory takes precedence over
the directory specified in the filename argument, although the acutal file *name*
( i.e. as returned by <code>os.path.basename()</code> ) will be maintained.
<br><br>
So if the <b>CHIMERA_IMAGE_DIR</b> environment variable is set to 
<br>
&nbsp;&nbsp;&nbsp;<i>/home/dan/chimera-images</i> 
<br><br>
and the filename argument passed to the snapshot function is 
<br>
&nbsp;&nbsp;&nbsp;<i>/tmp/chimera-ts-images/small_mol.png</i>
<br><br>
<code>snapshot</code> will actually save the file to
<br>
&nbsp;&nbsp;&nbsp;<i>/home/dan/chimera-images/small_mol.png</i>
<br>
<p>This is to facilitate maintaining several sets of images, without having to
change the path in all the occurrences of the <code>snapshot</code> function.</p>

TestUtils.py also defines a function called <code>my_image_directory</code> which returns
the path to the 'standard' image comparison file-save location:
<br>
&nbsp;&nbsp;&nbsp;<code>CHIMERA/test/images/PLATFORM</code>
<br>
where <b>CHIMERA</b> is the root of your checked-out version of Chimera, 
and <b>PLATFORM</b> is the result of Python's sys.platform.
The path returned by this function can be used as the filename argument to the <code>snapshot</code> function
to save images into the 'standard' image comparison file-save location:
<pre>
   TestUtils.snapshot( os.path.join(TestUtils.my_image_directory(), 'small_mol.png') )
</pre>

<p>
<b>Note</b>, that if this is called from one of the Linux builds (RedHat8-X11 or Linux-X11),
my_image_directory() returns <i>CHIMERA/test/images/linux2</i> for the Linux-X11 build,
and <i>CHIMERA/test/images/redhat8</i> for the RedHat8-X11 build
</p>

<br>
<h4>Comparing images</h4>

<p>The <i>CHIMERA/test/tools/imgcmp.py</i> script is used to compare images saved during the test suite, and 
then generate several report files containing different views on the comparison results</p>

<br>
<b>USAGE</b>:
<pre>   imgcmp.py output_file directory1 directory2 ... directoryn</pre>

<b>DESCRIPTION</b>:
<pre>   output_file    file will contain the main 'image comparison by platform' html page

   directory1 directory2 ... directoryn
          list of directories from which to compare images. The script will only compare 
          those files which occur in *all* the directories given. 
</pre>
<b>Example Usage:</b>
<pre>
    <b>%</b> ./imgcmp.py /usr/tmp/dan/testsuite-comp/comp_results.html /usr/tmp/dan/testsuite-imgs/{windows_imgs, linux_imgs, irix_imgs}
     
       This will find all similarly named files in the three directories, and run the
       comparison on them, printing out the results to the file <i>/usr/tmp/dan/comp_results.html</i> . For example,
       if there is a file named <i>molecule.png</i> in all three directories, this file will be included in the 
       comparison. However, another file named <i>mol_surf.png</i> that only occurs in two of the named directories,
       would not be included in the comparison results </i> . 
</pre>


<p>The 'main' view (i.e. the file that gets written to the <code>output_file</code> argument to the script)
shows image comparison results for each file, for each combination of platforms.
Assuming that imgcmp.py was used to compare the three files <i>large_color.png</i>, <i>small_color.png</i>,
and <i>session.png</i> across three platforms <i>linux2</i>, <i>irix6</i>, and <i>win32</i>, the output would look like:<p>
<br>
<!-- start example table -->

<table border="1" cellspacing="0" cellpadding="2">
<tr>
<th align="center" valign="middle">File</th>
<th align="center" valign="middle">win32<br>irix6</th>
<th align="center" valign="middle">win32<br>linux2</th>
<th align="center" valign="middle">irix6<br>linux2</th>
</tr>
<tr>
<td align="center"><a href="per-image/large_color.png.html">large_color.png</a></td>
<td align="left">
<a href="side-by-side/large_color_win32_irix6.html">view images</a>
<br>
R:0.35&plusmn;0.7<br>
G:0.35&plusmn;0.71<br>
B:0.36&plusmn;0.7<br>
</td>
<td align="left">
<a href="side-by-side/large_color_win32_linux2.html">view images</a>
<br>
R:0.51&plusmn;0.78<br>
G:0.48&plusmn;0.79<br>
B:0.51&plusmn;0.8<br>
</td>
<td align="left">
<a href="side-by-side/large_color_irix6_linux2.html">view images</a>
<br>
R:0.22&plusmn;0.47<br>
G:0.19&plusmn;0.45<br>
B:0.22&plusmn;0.47<br>
</td>
</tr>
<tr>
<td align="center"><a href="per-image/session.png.html">session.png</a></td>
<td align="left">
<a href="side-by-side/session_win32_irix6.html">view images</a>
<br>
R:<font color="red">7.1</font>&plusmn;<font color="red">28</font><br>
G:<font color="red">9.6</font>&plusmn;<font color="red">34</font><br>
B:<font color="red">7.2</font>&plusmn;<font color="red">28</font><br>
</td>
<td align="left">
<a href="side-by-side/session_win32_linux2.html">view images</a>
<br>
R:<font color="red">7.3</font>&plusmn;<font color="red">25</font><br>
G:<font color="red">7.8</font>&plusmn;<font color="red">26</font><br>
B:<font color="red">7.5</font>&plusmn;<font color="red">25</font><br>
</td>
<td align="left">
<a href="side-by-side/session_irix6_linux2.html">view images</a>
<br>
R:<font color="red">10</font>&plusmn;<font color="red">31</font><br>
G:<font color="red">12</font>&plusmn;<font color="red">35</font><br>
B:<font color="red">10</font>&plusmn;<font color="red">31</font><br>
</td>
</tr>
<tr>
<td align="center"><a href="per-image/small_color.png.html">small_color.png</a></td>
<td align="left">
<a href="side-by-side/small_color_win32_irix6.html">view images</a>
<br>
R:0.23&plusmn;0.57<br>
G:0.23&plusmn;0.58<br>
B:0.23&plusmn;0.57<br>
</td>
<td align="left">
<a href="side-by-side/small_color_win32_linux2.html">view images</a>
<br>
R:0.34&plusmn;0.67<br>
G:0.31&plusmn;0.66<br>
B:0.34&plusmn;0.67<br>
</td>
<td align="left">
<a href="side-by-side/small_color_irix6_linux2.html">view images</a>
<br>
R:0.15&plusmn;0.4<br>
G:0.12&plusmn;0.38<br>
B:0.14&plusmn;0.4<br>
</td>
</tr>
</table>

<!-- End example table -->
<br>
<p>
Each file name on the left is linked to another page that has a cross-platform comparison. The page
for <i>large_color.png</i> has a table like this:

<br><br>
<!-- Start per-image comparison page -->
<table border="1" cellspacing="0" cellpadding="2">
<tr>
<th align="center" valign="middle">Platform</th>
<th align="center" valign="middle">win32</th>
<th align="center" valign="middle">irix6</th>
<th align="center" valign="middle">linux2</th>

</tr>
<tr>
<td>win32</td>
<td align="left">
R:0&plusmn;0<br>
G:0&plusmn;0<br>
B:0&plusmn;0<br>
</td>
<td align="left">
R:0.35&plusmn;0.7<br>

G:0.35&plusmn;0.71<br>
B:0.36&plusmn;0.7<br>
</td>
<td align="left">
R:0.51&plusmn;0.78<br>
G:0.48&plusmn;0.79<br>
B:0.51&plusmn;0.8<br>

</td>
</tr>
<tr>
<td>irix6</td>
<td align="left">
R:0.35&plusmn;0.7<br>
G:0.35&plusmn;0.71<br>
B:0.36&plusmn;0.7<br>
</td>
<td align="left">

R:0&plusmn;0<br>
G:0&plusmn;0<br>
B:0&plusmn;0<br>
</td>
<td align="left">
R:0.22&plusmn;0.47<br>
G:0.19&plusmn;0.45<br>

B:0.22&plusmn;0.47<br>
</td>
</tr>
<tr>
<td>linux2</td>
<td align="left">
R:0.51&plusmn;0.78<br>
G:0.48&plusmn;0.79<br>
B:0.51&plusmn;0.8<br>

</td>
<td align="left">
R:0.22&plusmn;0.47<br>
G:0.19&plusmn;0.45<br>
B:0.22&plusmn;0.47<br>
</td>
<td align="left">
R:0&plusmn;0<br>
G:0&plusmn;0<br>

B:0&plusmn;0<br>
</td>
</tr>
</table>
<!-- End per-image comparison page -->
<br>


<p>Finally, each cell in the table is linked to a page showing a side-by-side comparison of the
two images being compared in that cell. The <i>'view images'</i> link  in the <i>large_color.png</i> cell  for the win32/irix6 
comparison leads to a page like this:</p>
<br>
<!-- start side-by-side page -->
<table>
    <tr>
      <td>
        <h4> Image 'large_color' on win32, generated 2004-05-14 15:14:53 </h4>

        <img src="image-comparison/images/win32/large_color.png">
      </td>
      <td>
        <h4> Image 'large_color' on irix6, generated 2004-05-14 15:14:53 </h4>
        <img src="image-comparison/images/irix6/large_color.png">
      </td>
    </tr>
    </table>
<!-- end side-by-side page -->

<br><br>
<h4><i>imgcmp.py</i> output file tree structure</h4>
In addition to writing the <i>output_path</i> file (passed to the script as as argument) which contains
the image comparisons by platform information, <i>imgcmp.py</i> also generates many more files containing 
other views of the results. The directory containing the file specified in the <i>output_path</i> argument 
is used as the root directory for this set of files. As the <i>imgcmp.py</i> script automatically generates 
the directories and files described here, it is not necessary to know anything about them. This documentation 
is here mostly for the sake of being thorough.
<br>
<br>
It is organized as follows:
<br>
&nbsp;&nbsp;(<b>Note: </b> the term <b>OUTPUT_DIR</b> will be used to refer to the directory that contains
the file OUTPUT_PATH)
<br><br>
<b>OUTPUT_PATH</b>
<pre>     Passed in as an argument to <i>imgcmp.py</i>. This file will contain the top level
     table showing image comparison results for each file, in all combimations of platform
     pairs.
</pre>
<b>OUTPUT_DIR/per-image/</b>
<pre>     This directory will contain one html file for each image included in the comparison. Each
     of these files contains an html table showing comparison results for that image. 
</pre>
<b>OUTPUT_DIR/side-by-side/</b>
<pre>     Side-by-side image comparisons. Each file shows the same image from two different platforms.
</pre>
<b>OUTPUT_DIR/images/</b>
<pre>     This directory will contain a subdirectory for each platform in the comparison 
     (i.e. each directory passed as an argument to the <i>imgcmp.py</i> script). In each
     of these subdirectories will be a symbolic link to the actual location of the image file
     on the filesystem. Html files in the <i>side-by-side</i> directory use these link
     to display the image files.
</pre>

</body>
</html>
